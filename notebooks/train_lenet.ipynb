{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "gN_ecPftr174"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2q_pHeGICtEj"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/image_classification_course"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "8RwjqDGygn8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Steps to Train an Image Classfiction model**\n",
        "\n",
        "\n",
        "1.   Loading the train and test dataset\n",
        "2.   Loading the model\n",
        "3.   Hyper-Parameters\n",
        "4.   Training Loop\n",
        "5.   Check the model accuracy\n",
        "6.   Saving the model checkpoint or weight\n",
        "\n"
      ],
      "metadata": {
        "id": "dmmLGZKSoF_c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Package**"
      ],
      "metadata": {
        "id": "iM_jqLaVhIPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from time import sleep\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm import tqdm\n",
        "\n",
        "from dataset import load_data\n",
        "from models.lenet import LeNet\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "metadata": {
        "id": "XMYYLZC-gfUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)\n"
      ],
      "metadata": {
        "id": "01Rf_3EMrnNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading the train and test dataset**"
      ],
      "metadata": {
        "id": "wof4wzewhRUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = 'data/version1'\n",
        "image_size = (32,32)\n",
        "batch_size = 16\n",
        "\n",
        "train_data, test_data = load_data(dataset_path, image_size, batch_size)"
      ],
      "metadata": {
        "id": "BUWU5HV9gtKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loading the model**"
      ],
      "metadata": {
        "id": "3NGkTg1KhQbn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "in_channel = 3\n",
        "num_classes = 7\n",
        "\n",
        "model = LeNet(in_channel= in_channel, num_classes= num_classes)\n",
        "\n",
        "model = torch.compile(model)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "model.train()"
      ],
      "metadata": {
        "id": "M4OVCp_Hg1MV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyper parameters loss and optimizer**"
      ],
      "metadata": {
        "id": "pvNdUusThcRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "num_epochs = 30\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
      ],
      "metadata": {
        "id": "hxdTGA40g4hQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model accuracy check**"
      ],
      "metadata": {
        "id": "lAoR0HqFiKHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_accuracy(test_data, model):\n",
        "    # Initialize the prediction and label list\n",
        "    y_pred = np.zeros(0)\n",
        "    y_true = np.zeros(0)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for images_batch, y_true_batch in test_data:\n",
        "            images_batch = images_batch.to(device)\n",
        "\n",
        "            scores = model(images_batch)\n",
        "            _, y_pred_batch = scores.max(1)\n",
        "            y_pred_batch = y_pred_batch.cpu()\n",
        "\n",
        "            y_pred = np.concatenate((y_pred, y_pred_batch))\n",
        "            y_true = np.concatenate((y_true, y_true_batch))\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    report = classification_report(y_true, y_pred)\n",
        "    print(report)"
      ],
      "metadata": {
        "id": "tNvW9Iz9iDaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training loop**"
      ],
      "metadata": {
        "id": "XuKVh9HwhjnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, num_epochs + 1):\n",
        "    pbar_batch = tqdm(train_data, unit=\"batch\")\n",
        "    losses = []\n",
        "    for data in pbar_batch:\n",
        "        pbar_batch.set_description(f\"Epoch {epoch}\")\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        scores = model(images)\n",
        "        loss = criterion(scores, labels)\n",
        "        losses.append(loss.item())\n",
        "        cost = sum(losses)/len(losses)\n",
        "\n",
        "        # backward\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        pbar_batch.set_postfix(loss=cost)\n",
        "        sleep(0.1)\n",
        "\n",
        "    if (epoch % 5) == 0:\n",
        "        get_model_accuracy(test_data, model)\n",
        "        sleep(0.1)\n",
        "\n",
        "\n",
        "print(\"model training complete\")"
      ],
      "metadata": {
        "id": "b2Sh9-ObhGPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8ZSc7bvvFUjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Saving the model checkpoint**"
      ],
      "metadata": {
        "id": "OI561TPMhpt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = \"weights/lenet\"\n",
        "if not os.path.exists(model_save_path):\n",
        "    os.makedirs(model_save_path)\n",
        "\n",
        "torch.save(model.state_dict(), model_save_path + \"/lenet_model_checkpoint.pth\")"
      ],
      "metadata": {
        "id": "0YSxfRE8g-w6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WMddLkM2gmRK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}